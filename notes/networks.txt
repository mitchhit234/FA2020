8-24

The internet is billions of connected computing devices of three types:
	
	Computing Devices:
	Hosts/end systems, they run network apps at internet's "edge"

	Routers:
	Packet switches: forward "packets" between hosts (chunks of data)
	Ex: routers, switches

	Links:
	Communication links (wired and wireless)
	Ex: fiber, copper, radio, satellite
	Transmission rate: bandwith


Networks are a collection of devices, routers, and links, managed by an organization

The internet is essentially a network of networks (Interconnected ISPs)





The internet is also an infrastructure that provides services to applications

Protocols control sending and reciving of messages
Ex: HTTP, ICP, IP, WiFi, 4G, Ethernet

Internet standards
RFC: Request for comments
IETF: Internet Engineering Task Force


Network core consists of interrconnected routers
Ex: local/regional or national/global ISPs (11)



The internet provides a programming interface to distributed applications

Hooks allow sending/reciving of apps to connect to and use internet transport service

Provides service options, similar to postal service



Back to the internet structure:

Connect end systems to edge routers with:
Residential access nets
Institutional acces networks (school, company)
Mobile access networks (WiFi, 4G, 5G)

Tranmission rate and shared/dedicated access among users effect performance





8-26

Frequency division multiplexing (FDM) are different channels transmitting different frequency bands


Acces via cable based: (14)
Hybrid fiber coax (HFC) has up to 1.2 Gbs down, up to 100 Mbps up

Acces via digital subscriber line: (13)
use existing telephone line to central office DSLAM
up to 50 Mbps down, 16 Mbps up



Wireless access networks is shared among multipule end systems and connects them to the router

Wireless local area networks (WLAN) are typically within or around building (100ft)

Wide area celluar access networks are provided by celluar networks, typically 10km radius, slower transmission rate than WLANs


Enterprise networks are operated by companies, universities, etc
Mix of wired, wireless linked technologies, connecting multipule different switches and routers





Host sending function:
takes application message,
breaks into smaller chuncks, known as packets, of length L bits,
transmits packet into access network at link transmission rate R

packet transmission delay = L/R (bits/bits per second)



Links: physical media

bit propagates between transmitter and reciever
the physical link is what lies between transmitter and reciever
guided media is when signals propogate in a solid media (copper, fiber, coax)
unguded media is when signals propogate freely (radio, WiFi)

types of solid media:
twisted pair - two insulated copper wires (Cat 5/6)
coaxial cable - two concentric copper conductors, bidirectional, brodband. multipule frequency channels on cable, 100's Mbps per channel
fiber optic cable - glass fiber carrying light pulses, each pulse being a bit. high speed operation (100's Gbps), low error rate (immune to electromagnetic noise)

types of unguided media:
wireless radio - signal carried in electromagnetic spectrum. can be obstructed by the environment
terrstrial microwave, wireless LAN, wide area celluar, satellite





Packet-switching is when the hosts break application-layer messahes into packets
Transmission delay takes L/R seconds to transmit an L-bit packet at R bps
Store and forward: entire packet must arrive at router before it can be transmitted to the next link
End-end delay: 2L/R(above), assuming zero propagation delay

Packet queuing and loss: if arrival rate to link exceeds the transmission rate of link, packets will queue, waiting to be transmitted on output link OR packets can be dropped if memory buffer in router fills up

Forwarding: local action, move arriving packets from router's input link to appropriate router output link
Routing: global action, determine source to destination paths taken by packets



Circuit switching is an alternative to packet switching where end-end resources are allocated, and reserved for a call between the source and destination
Has dedicated resources, no sharing enables guaranteed performance
Circuit segment idle if not being used by that specific host
Not used that much today, but used in traditional telephone networks


Frequency Division Multiplexing (FDM) is like the way described above

Time Division Multiplexing (TDM) has slots divided by time, each call allocated can transmit at maximum rate of wider freqeucny bands, but only during its time slots






8-30


Packet switching vs Circuit switching

Packet switching allows more users to use the network
Ex: 1 Gb/s, each user has 100Mb/s when active, but only active 10% of the time

Circuit switching would allow up to 10 users
Packet switching could allow 35 users easily, the probability that there are more than 10 active users at a time is less than 0.4%

Packet switchintg is great for "bursty" data, but is also exposed to packet dealy and loss due to buffer overflow




Internet structure:

Hosts connect to internet via access ISPs
Access ISPs are connected to each other, so any two hosts can send packets to each other
Resulting network of networks is very complex, its evolution was driven by economics and national policies

How do ISPs connect to each other?

Connecting each asccess ISP to each other directly doesnt scale well [O(n^2)]

One option is connecting each access ISP to one global transit ISP. Customer and provider ISPs work with each other to accomplish that
There are more than one global ISP however; the market is competitive

Global ISPs connect via an IXP (internet exchange point) or peering link (direct connecting between 2 global ISPs)
Regional ISPs may also arise to connect cunsomer ISPs to Global ISPs

Then Content provider networks (Google, Microsoft) may run their own network 
to bring services and conent close to end users, bypassing tier 1 and even regional ISPs




Packet delay, the four soucres:

Nodal delay = d_proc + d_queue + d_trans + d_prop

nodal processing: check bit errors, determine output link, typically < ms

queueing delay: time waiting at output link for transmission, depends on cogestion level of router

transmission delay: packet length divided by transmission rate (bps) (L/R)

propagation delay: length of physical link divided by propagation speed (2x10^8 meters per second) (d/s)


Throughput: rate at which bits are being sent from sender to reciever

instantaneous: rate at a given point and time
average: rate over longer period of time





8-31

Networks are complex with many pieces
- hosts
- routers
- links of various media
- applications
- protocols
- hardware, software


Layers: each layer implements a service cia its own internal-layer actions. They rely on services provided by layers before
Explicit structure allows identification, relationship of complex system's pieces
Modularization eases maintenance, updating of system
Changes in layer's service implementation is transparent to the rest of the system


Internet Protocol Stack is each layer put together (51)

- Application Layer
Supporting network applications (IMAP, SMTP, HTTP)

- Transport Layer
Process-process data transfer (TCP, UDP)

- Network Layer
Routing of datagrams from source to destination (IP, Routing Protocols)

- Link Layer
Data transfer between neighboring network elements (Ethernet, 802,11, WiFi, PPP)

- Physical Layer
Bits "on the wire"





Internet was not originally designed with much security in mind. It was thought that it would contain a group of mutually trusting users attached to a transparent network
Internet protocl designers have been trying to patch this issue


Malware can get in to a host from:

A virus - self replicating infection by receiving and executing object (EX: email attachment)
A worm - self rplicating infection by passively receiving object that gets itself executed

Spyware malware can record keystrokes, web sites visited, and upload info to a collection site

Infected host can be enrolld in botnet, used for spam or distributed denial of service (DDos) attacks

Denial of Service attackers make resources unavaliable to legitimate traffic by overwhelming the resource with bogus traffic

Packet "sniffing" is reading/recording all packets passing by 

IP spoofing sends packets with a false source address





9-2

Application Layer Chapter


Creating a network app:
- runs of different end systems
- communicates over network

No need to write software for network core devices. They do not run user applications. Putting applications on end systems allows for rapid app development and propagation


Application architectures:
- Client Server (includes data centers/cloud computing)
- Peer to Peer (P2P)
- Hybrid of client and P2P


Client Server
Server:
- always on host
- permenant IP address
- often in data centers, scalable

Client:
- contact and communicate with server
- may be intermittently connected
- may have a dynamic IP address
- do not communicate directly with other clients

Examples: HTTP, IMAP, FTP



Peer to Peer:
- no always on server
- abitrary end systems directly communicate
- peers request service from other peers, provide service in return
- peers are intermittently connected and change IP addresses

Examples: P2P File Sharing



Examples of Hybrid Systems: Skype, Zoom, IM




Process communications
Process: program running within a host

Within a host, two processes communicate using inter process communication

Processes in different hosts communicate by exchanging messages

Client Process: process that initiates communication
Server Process: process that waits to be contacted



Processes send and recieve messages to/from its socket
Socket is similar to a door:
- Sending process shoves message out the door
- Sending process relies on transport infastructure on the other side of the door to deliver the message to the socket at receiving process
- Two sockets involved, one sending one receiving


Addressing processes
Example port numbers:
- HTTP Server: 80
- Mail Server: 25

Application layer protocol:
Defines type of messages exchanged (requests, response)
Defines message syntax
Defines message Semantics


Public Domain (open protocols) defined in RFC'sm everyone has access to protocol definition

Proprietary does not reveal the rules for their protocols (closed protocols)


As it comes to data loss, some apps require 100% reliable data transfer, while some other apps (like audio apps) tolerate some data loss

Some apps require low delay to be effective, so timing is an important variable

Some apps requrie a minimum amount of throughput to be effective


Internet transport protocols services

TCP Service:
- reliable transport
- flow control, sender wont overwhelm reciever
- congestions control, throttle sender when network overloaded
- does not provide timing, minimum throughput guarantee, security
- connection oriented, setup required between client and server processes

UDP Service:
- unreliable data transfer between sending and receiving process
- does not provide reliability, flow control, congestion control, timing, throughput, security, or connection setup


Securing TCP:

Security Socket Layer (SSL):
- provides encrypted TCP connections
- data integrity 
- end point authentication
SSL is implemented in the application layer


Total delay = nodal dealy
nodal delay = processing delay + queuing delay + transmission delay + propogation delay
transmission delay = L/R = packet length/transmission rate
propogation dealy = d/s = link length/propogation speed


Web and HTTP

Web page consists of objects stored on different Web Servers
Objects can be HTML files, Jpg images, java applet, audio file, etc.
Web page is addressed by a URL (protocol -> host name -> path name)

HTTP stands for hypertext transfer protocol
- Is an application layer protocol
- Client/Server model
Client requests, recieves, and displays web objects
Server sends objects in response to requests

HTTP uses TCP
- Client initiates TCP connection to server port 80
- Server accepts
- HTTP messages exchanged between browser and web server
- TCP connection closed

HTTP is stateless, meaning no information is kept on the server's side about past client requests

Persistent HTTP vs Non Persistent HTTP

Non-persistent HTTP only sends one object per TCP connection at a time. To request multipule objects requires multipule connections

Persistent HTTP leaves the TCP connection open after the server already sent the response. Usually the TCP connection will close after a certain time. Minamilizes the amount of RTT's in a transaction between a client and server



HTTP Request Messages, two types:
Request - ASCII (human readable) consists of request line, header lines, carriage return
Response - ASCII (human readable) consists of status line, header lines, data

Maintaining user/server state: cookies
Websites and client browsers use cookies to maintain some state between transactions
Four components of cookies:
- Cookie header line of HTTP response message
- Cookie header line in next HTTP request message
- Cookie file kept on user's host, managed by user's browser
- Back end database at Web Site's server

Web Caches (Proxy Servers)
Satisfy client requests without involving origin server
User configures browser to point to a web cache
Browser sends all HTTP requests to cache
If object is in the cache, cache returns object to the client, else cache requests object from origin server



Email

Three Major Components:
User agents - mail reader, composes edits and reads messages
Mail Servers - contains incoming messages, messaging queue, and SMTP protocol
Simple Mail Transfer Protocol

SMPT
Uses AT port 25
Direct transfering between servers
Three phases of transfer:
- Handshaking, Transfer, Closure
Command/response interaction is like HTTP

HTTP PULL, SMTP PUSH
Single part vs Multi part message
8 bit vs 7 bit

Mail Access Protocols:
Internet Mail Access Protocol (IMAP) - provides retreival, deletion, and folders
HTTP - Web based interface on top of SMTP, the IMAP deals with retrieving message info




Domain Name System (DNS)

Distributed database implemented in the hierarchy of many name servers
Implememnted as application-layer protocol
Hostname to IP address Translation
Host Aliasing
Mail Server Aliasing

Why not centralize DNS?
Single Point of Failure
Traffic Volume
Distant Centralized Database

Cenrtalized doesn't scale well, comcast dns servers alone have 600B queries a day


Hierarchial DNS Database
Root DNS Servers -> Top level domain (split up by .com, .edu, etc) -> Authorative 

Root Servers are an incredibley inportant internet function
Also provide DNSSEC - provides security

ICANN Internet Corporation for Assigned Names and Numbers manages root DNS domain

Iterative queries are considered best practice for DNS queries
SMTP uses TCP
DNS uses TCP and UDP


9-14

Chapter 3 Transport Layer

Sender breaks application messages into segements, passes it down to network layer. When the receiver gets it, reassembles segments and passes it to its application layer

Network layer provides logical communication between hosts

Transport layer provides logical communication between processes


9-16

When UDP clients send UDP segments to the same destination port number at a reciving host, those segments will always be directed to the same socket as the reciving host
However, this is not true for TCP clients. they can direct segments to different sockets

It is possible for two TCP segments with source port 80 to be sent by different processes at the ending host
This is not possible for UDP segments

Why is there UDP?
No connection establishment (No extra RTT delay)
No connection state at sender, reciever (simple and cheap)
Small header size (less bandwith needed)
No congestion control (no speed limit)

UDP is used in:
Streaming Multimedia (loss tolerant, rate sensitive)
DNS Name Queries for fastest service
SNMP

To make UDP reliable, add reliability at the application layer like QUIC, and add cogestion control at the application layer

UDP Checksum's goal is to detect errors in a transmittited segment
Sender treats contents of UDP segment as sequence of 16 bit integers
Checksum addition of segment content, value put in checksum field


Chapter 4 Network Layer

Services and Protocols
Sender rencapsulates segments into datagrams, passes to link layer
Reciever delivers segments to transport layer

In every internet device, there are hosts and routers

Two Key Network Layer Functions
Forwarding: Move packets from a router's input link to appropriate router output link
Routing: Determine the route taken by packets from source to destinations (Routing algorithms)

The data (local) plane determines how a datagram arriving on router input port is forwarded to router output port
The control (network wide) plane determines how a datagram is routed among routers along end to end paths from source to destination


Software Defined Networking (SDN) control plane connects routers and forwards important routing information

Network Layer Service Model
Internet has a best effort service model, no QoS guarantees

Simplicity of mechanisim has allowed internet to be widely adopted
Sufficent provisioning of bandwith allows performance of real time application to be good enough most of the time
Replicated application layer distributed services connecting close to clients networks allow services to be provided from many locations






Book Notes Chapter 3: Transport Layer

Transport Layer provides logical communication between application processes running on different hosts
Network Layer provides logical communication between hosts
Extending the network layers communication from hosts to processes on those hosts is called transport layer multiplexing and demultiplexing

Transport Layer is only implemented in end systems

The internet's Transport Protocols are UDP (User Datagram Protocol) and TCP (Transmission Control Protocol)
UDP provides an unreliable, connectionless service to the application layer
TCP provides a reliable, connection oriented service to the application layer

Transport Layer packets are called segments, Network Layer packets are called datagrams

The Internet's Network Protocol is called IP (Internet Protocol)
IP provides logical communication between hosts with a best-effort delivery service model (meaning it is unreliable)
TCP's reliable service is an example of a service a transport layer protocol can provide that a network layer protocol doesnt already


UDP provides:
Multiplexing/Demultiplexing
Intergrity Checks

TCP provides:
Multiplexing/Demultiplexing
Intergrity Checks
Reliable Data Transfer (making sure all data is delivered and in order):
	- Flow Control
	- Sequence Numbers
	- Acknowledgements
	- Timers
Congestion Control


Multiplexing and Demultiplexing:

Transport Layer Packet will contain a source port # and dest port # (each 16 bits) to direct network packets to the correct port
Port numbers ranging from 0 to 1023 are called well known port numbers and are restricted, meaning they are reserved for well known protocols
EX: HTTP uses port 80, FTP uses port 21
http://www.iana.org [RFC 3232]

UDP Segment includes source port and destination port, TCP includes this information plus source IP and destination IP

Two arriving TCP segments with different source IP addresses or source port numbers will be directed to two different sockets, unlike UDP
EX: Host A and Host B both send a segment to port 80 on Host (Server) C. Transport Layer demultiplexing allows these segments to be sent to unique sockets, while UDP would not
Super useful in cases like web servers, where every client is sending HTTP requests to server port 80


Connectionless Transport: UDP

Reasons one might choose to use UDP instead of TCP:
Bypass congestion control, resulting in quicker sending of segments
Can sacrafice data loss for less delays (TCP continues to send over and over until reliable delivery is achieved)
No handshaking like in TCP, resulting in less delay (biggest reason DNS uses UDP)
Reliable data transfer can be acheived in the application level (EX: Google Chrome's QUIC protocol)
No connection state variables required, reducing delay and segment size
Smaller packet header overhead, TCP has 20 bytes while UDP only has 8

Realtime applications in general do not respond to TCP well due to the above reasons

Examples of some popular internet applications and what they use:

Email 					SMTP 					TCP
Remote Terminal Access 	Telnet					TCP
Web 					HTTP 					TCP
File Transfer 			FTP 					TCP
Remote File Server 		NFS						UDP (usually)
Multimedia 				proprietary (usually)	UDP, sometimes 	TCP
Internet Telephony 		propreitary (usually) 	UDP, sometimes 	TCP
Network Management 		SNMP 					Typically 		UDP
Name Translation 		DNS 					Typically 		UDP


Principles of Reliable Data Transfer

In a reliable data transfer, no data bits are corrupted, lost, or delivered out of order. TCP is an example of a reliable data transfer protocol

Types of RDT protocols:

rdt1.0 - Reliable data transfer over a perfecty reliable channel

rdt2.0 - Reliable data transfer over a channel with bit errors
Uses error detection (via checksums for rdt2.0)
Reciever Feedback via positive acknowledgement (ACK) or negative acknowledgement (NAK), "OK" or "Please repeat that" represented as 0 and 1
Retransmission by sender if it recieves an NAK from the reciever (1)

Reliable data transfer protocols based on transmission are called ARQ (Automatic Repeat reQuest) protocols
Since rdt2.0 sender stops and waits for a reciever response, it is called a stop and wait protocol. During that stopping period it can't even get information from its own application layer

rdt2.0 has a fatal flaw, where is can not handle the errors that could occur if the reciever's acknowledgement packet got corrupted. However rdt2.1 fixes that (but still assumes all packets are not lost)

rdt2.1 - Reliable data transfer over a channel with bit errors (bidirectional)
New data field added to segments called the sequence number
Receiver checks sequence number to see if it is a new transmission or old transmission. With this info, it knows what the sender recieved as its acknowledgement
Reciever will still send a negative acknowledgement for a corrupted packet, however, for an out of order packet, it will send a positive acknowledgement
A sender that recieves dupliate ACKs of the same package will know that the reciever did not recieve the packet they sent after that original ACK

rdt 2.2 - Reliable data transfer over a channel with bit errors (bidirectional)
Reciever now also includes a reciever number in its acknowledgement packets
No longer requires use for NAK. Just send an ACK and the sequence number

rdt3.0 - Reliable data transfer over a channel with bit errors and possible loss bits (bidirectional)
Can be implemented in many ways, in rdt3.0, it is the sender's responsibility
Sender uses its own judgement to choose a time where it will resend a segment if it has not recieved an ACK
This implementation requires a countdown timer that can interrupt a sender after a period of time has elapsed
Sender starts timer for each packet, responds to any timer interrupts, and stops the timer appropriatley 
Rdt3.0 also known as the alternating bit protocol (pg 246 for good diagram)


Rdt3.0 is a functionally correct protocol, however, since its a stop and wait protocol, most of today's high speed networks would not be satisfied with its performance
The utilization of the sender is very low, since it stops and waits between the time it sends its last byte, and the time it recevies an acknowledgement (or timeout). Between then, the propigation delay, process of the reciever, any buffers/delays, and propigation delay back (acknowledgement transmission rate is negligeble) all take place and take up a majority of the time

Pipeling helps this problem, allowing the sender to send multipule packets in a row before it ever gets an acknowledgement back from the reciever
Pipelining has some consequences though, such as:
Range of sequence numbers must be increased 
Sender/reciever may have to buffer more than one packet


Go Back N protocol (GBN):
Sender is allowed to transmit multipule packets before receiving acknowledgements
Restrained to a size of N packets for reasons tied to congestion control
Sequence numbers in a GBN fall into four categories:
Already ACK'd [0-base-1]
Sent but not ACK'd [base-nextseqnum-1]
Usable but not sent [nextseqnum, base+N-1]
Not useable [base+N or greater]

N is refered to as the window size and the GBN protocol itself is called a sliding window protocol

GBN uses cumulative acknowledgements, meaning that if the sender recieves an ACK for the segment with sequence number x, it can assume all segments with sequence numbers less than x have also been recieved. This is because the sender always sends one packet at a time, and the reciever will always just send an ACK for its last successful packet recieved if there is an error (and discards error filled packet, even if its just delivered out of order)



Selective Repeat (SR):

A downside to GBN is that a single segment error can result in many segments having to be redelievred (if the window size and bandwith delay is very high)
SR helps mitigate this, by having the sender retransmit only those specific segments that have errors

Differences between SR and GBN:
SR has logical timers for each individual packet, unline GBN, which just had a timer for its last non-acknowledged packet
If an ACK is recieved in SR, and the sequence number is equal to the current base of the window, the window base is moved forward to the unacknowledged packet with the smallest sequence number
Receiver side will only send packets up to its application layer for up to the lowest unacknowledged sequence number (EX: receiving 3, 4, 5, then 2, it will send once 2 comes in in the order 2 3 4 5)
SR reciever must also send an ACK to the sender even if it falls before the base of the current reciever window (pg 257 for good diagram)
SR protocol must have a windows size that is less than or equal to half of the range of sequence numbers (258)

One last note on the assumption of in order data delievery, sequence numbers must not only be used, but be used correctly in order to assure in order data delivery. This involves an assumption made about a packet's "lifetime" in the network (For example, TCP assumes a packet's maximum lifetime in a network is 3 minutes)

Table 3.1 on pg 260 is a good recap for reliable data transfer terms




Connection Oriented Transport (TCP):

Connection oriented means that two processes must "handshake" before they start transmiting data between each other
TCP provides a full duplex service, data from A to B can flow at the same time as data flows from B to A
TCP is a point to point connection, meaning there is no multicasting (one to many) connections with TCP

The maximum amount of data that can be placed in a segment is called the maximum segment size (MSS)
MSS is usually determined by the largest link layer frame that can be sent by the local host called the maximum transmission unit (MTU)

TCP header field is typically 20 bytes (12 bytes more than UDP header field)
TCP segment includes:
Source port and Dest port #'s (4 bytes)
Sequence number field (4 bytes) and acknowledgment number field (4 bytes) are both used for reliable data transfer procedures
Header length field (4 bit) specifies length of TCP header (needed because sometimes the options field is not present)
Options field is used when the reciever andsender negotiate the MSS 
The flag field contains 6 bits, one for each flag:
	ACK used to indicate whether value in acknowledgement field is valid
	RST, SYN, FIN used for connection setup and tear down
	CWR and ECE are used in congestion control
	The next two bits are not used but are mentioned for some reason
	PSH bit determines whether data is passed to upper layer immediatley
	URG marks the segment as urgent. The location of the last byte of urgent data is determined by the value in the urgent data pointer (4 byte)


When TCP breaks up a file into segments, it gives each segment a sequence number based on the # of byte that segment starts on
EX: file consists of 500,000 bytes, the MSS is 1,000 bytes, the first segment's sequence number is 0 (for illistrative purposes, the starting point is actually randomly assigned), the second is 1000, etc.

As for acknowledgment numbers, the acknowledgement number a server will give a client in its acknowledgement segment is the next sequence number it expects from the client in that client's next segment

TCP uses cumlulative acknowledgements. As for what it does with out of order recieved packets, that is up to the developer implementing TCP. Usually though, they keep the out of order bytes and wait for missing bytes to fill in the gap, to take better advantage of network bandwith


TCP calculates an EstimatedRTT to determine the best values for its timers that maximize data rate and correct lost packet detection
SampleRTT is a value given once each RTT (not once each segment) of a paticular segment's actual RTT

EstimatedRTT = (1 - a) * EstimatedRTT + a * SampleRTT

a is used as a weight variable, the most common value of a being 1/8
This weighted average of SampleRTTs places more weight on recent RTT's than old ones. In stats, this is called an exponential weighted moving average (EWMA)

TCP also calculates a value, DevRTT, fot the deveation of the SampleRTTs

DevRTT = (1 - B) * DevRTT + B * abs(SampleRTT - EstimatedRTT)

DevRTT is a EWMA for the difference between SampleRTT and EstimatedRTT. The larger the variation, the larger the value of DevRTT

TCP's final timeout interval uses these two values to derive its own value

TimeoutInterval = EstimatedRTT + 4 * DevRTT

Before these two values are able to be calculated, the default TimeoutInterval is reccomended to be one second

As for how TCP handles timers, it doesn't have a timer variable for each individual packet, rather it keeps and updates its timer to be associated with the oldest unacknowledged segment
TCP also doubles the timer interval for a segment whenever it has to be retransmitted (if the timer already expired once before). This also contributes to congestion control

Often times a TCP sender will be able to tell if a packet was lost before timeout. This is because the reciever will most likely have sent a duplicate ACK (an ACK for which the sender has already recieved an earlier acknowledgement). Since TCP does not have negative acknowledgement, the receiver sends an ACK for its last in order recieved segment when it gets a segment that does not have its next expected sequence number
Table 3.2 on 278 has a list of what the reciever's response is to certain packets being sent

If three duplicate ACKs are recieved, the TCP sender performs a fast retransmit, retransmitting the missing segment before that segment's timer expires

TCP's error recovery system ends up being a hybrid of GBN and SR



Flow Control
While congestion control and flow control both acheive their goals by throttling the sender, their goals are different. The goal of congestion control is to limit congestion in the IP network, while flow control is used to eliminate the possibility of the reciever's buffer being overflowed by the sender

TCP accomplishes flow control by having the sender keep track of a variable called the recieved window. This variable represents how much free buffer space is avaliable at the reciever

rwnd = RcvBuffer - [LastByteRcvd - LastByteRead]

The rwnd variable (the amount of room left in the receive window) is sent from the reciever to the sender in every segment (is a part of the header file)
LastByteRcvd is the number of the last byte in the data stream that has arrived from the network and been placed in the recieve buffer at B
LastByteRead is the number of the last byte in the data stream that has been passed to the application layer
RcvBuffer is the max size of the reciever's recieve buffer

The sender in turn keeps track of two variables, LastByteSent and LastByteAcked
Taking the difference of those two variables (amount of unACKd data), the sender's goal is to make sure that difference always stays less than the rwnd variable it recieves from the recieving host. Then, it is guaranteed to not overflow the recieve buffer with its segments

If rwnd ends up being 0 at some point, there is no longer any incentive for the hosts to send segments to each other. Thats why TCP still forces the sender to send segments with one data byte to the reciever, so the reciever will have a reason to respond with ACKs, which will have updated rwnd values in their headers



TCP Connection Management
The steps below are how a TCP connection is established in a three way handshake method
1. Client side sends a special TCP segment to the server. This segment contains no application data. Its SYN flag bit is set to 1, and the client randomally chooses an initial sequence number called client_isn and puts it in the sequence number field
2. If the SYN segment arrives at the server, the server allocates its buffers and variables for the connection, then sends a connection granted segment to the TCP client (this is the vulnerability DDOS attacks take advantage of in the form of a SYN flood attack. pg 288 for the counter measure, SYN cookies). This connection granted segment also has no application data, its SYN bit is set to 1, its ACK field is set to client_isn+1, and the server also randomly chooses its own initial sequence number, server_isn, and puts it in the sequence number field. This segment is refered to as a SYNACK segment
3. Upon recieving a SYNACK segment, the client allocates its buffer and variables, then sends its own ACK to the server and puts server_isn+1 in its acknowledgement field. The SYN bit is now set to zero since the connection is established.

A similar process is used for closing a TCP connection. If the client is closing, it will send a segment to the server with the FIN flag set to one. The server will send an ACK segment, as well as a FIN segment of its own. The client will acknowledge the FIN segment, then the connection will be closed

Figure 3.41 and 3.42 has a diagram of TCP states that the client and server will go through in the lifetime of their connection (pg 286)

If a TCP server recieves a SYN segment that contains a destination port # that is not currently listening on the server, then the server will send a segment back with the RST flag bir set to 1. This is the server telling the client "I don't have a socket for that segment. Please don't resend the segment"



Principles of Congestion Control
There are 3 major causes of congestion in a network:
1. As packet arrival rates near the link capacity, large queuing delays are experienced
2. Packets are needlessly transmitted, causing routers to use precious link bandwith to forward a packet that did not need to be forwarded
3. Packets can be forwarded along multipule different points on a path, but they may suddenly run into a hotly contested point where packets are being lost to buffer overflow. This results in every single point leading up to that having its work wasted forwarding that packet.


The offered load is the rate at which data is sent, including retransmitted segments

There are two broad approaches to congestion control:
1. End to end congestion control, where no data is passed from the network layer to the transport layer, is the approach TCP implements (since IP does not provide explicit support to the transport layer). The presence of congestion must be infered by the transport layer based off network activity, such as packet loss and delay. 
2. Network assisted congestion control, where the network layer will pass information to the transport layer to specifically point out congestion. Used in protocols like ATM's Avaliable Bit Rate (ABR), where a router informs the sender of the maximum host sending rate the router can support on an outgoing link




TCP Congestion Control

In addition to all the variables we talked about before, the TCP sender also stores a variable for congestion control called the congestion window
cwnd is very similar to rwnd
LastByteSent - LastByteAcked <= min{cwnd, rwnd}
Just like with rwnd, the sender's rate is going to be something around
cwnd/RTTbytes/sec

A TCP sender can percieve congestion via a loss event
A loss event consits of a timeout or receiving three duplicate ACKs
TCP is also self clocking, meaning it will use the rate at which it recieves non-dupliacte acknowledgements as the means to increase its congestion window size

TCP uses the following principles when it comes to detecting and responding to congestion:
1. A loss segment implies congestion, and the TCP sender's rate should be decreased when a segment is lost
2. A non duplicate ACK indicates that the network is delivering segments to the reciever, and that the sender's rates can be increased 
3. TCP bandwith probes, meaning that it will steadily increase its congestion window size until it runs into a loss event, dial back the cwnd, then start to steadily increase it again

These principles all play into the TCP congestion control algorithim, which has three major components:
1. Slow start
2. Congestion avoidance
3. Fast recovery

Slow start algorithm starts the cwnd value at 1 MSS, and increases the windows size 1 MSS for every non-duplicate ACK it recieves. Thus, the TCP send rate starts out small but grows exponentially
Once a loss event occurs, a couple of different scenarios can occur depending on the implementation. 
One is the cwnd reverts back to 1 MSS, and the variable ssthresh (slow start threshold) is set equal to cwnd/2 (the cwnd before it was reverted to one). 
Another way is cwnd can approach and then equal ssthresh. When this happens, it could be reckless to keep doubling the cwnd size, so slow start could pivot over to congestion avoidance, where it increases cwnd more cautiously
Also if the loss event is three duplicate ACKs, slow start could transition to fast recovery mode
The figure 3.51 on page 301 has an in depth chart for TCP slow start

In congestion avoidance, TCP increases the congestion window size once every RTT instead of once every ACK segment like slow start does
When congestion avoidance recieves a timeout, it goes back to slow start. When it recieves three duplicate ACKs, it goes to fast recovery

When entering fast recovery, ssthresh is set the same way, but cwnd is set differently
cwnd = ssthresh+3 * MSS , the plus three is used to account for the three duplicate packets
In fast recovery, the cwnd is increased by 1 MSS for each duplicate ACK that is recieved of the same sequence number. Once a new ACK is recieved, the algorithm goes back to congestion avoidance. If there is a timeout, the algorithm goes back to slow start

An earleir version of TCP called TCP Tahoe did not have this fast recovery state. The newer version of TCP, TCP Reno, does however

TCP Splitting optimitzes response times of clients to servers by introducing persistent TCP connections betweens links in the server (for example, the database server and front end server for a website)

TCP congestion control is refered to as an additive increase multiplitative decrease (AIMD) form on congestion control

Many TCP implementations use this AIMD algorithm, but others like TCP vegas try to detect congestion before packet loss occurs (by monitoring RTTs) and lower the sender rate lineraly rather than multiplicativley

Under TCP's AIMD algorithm, we can calculate the average throughput of a connection to be
avg = (0.75 * W)/RTT, where W equals the window size when a loss event occurs
It can also be calculated as a function of L( loss rate), RTT, and MSS
avg = (1.22 * MSS)/(RTT * sqrt(L))


Fairness
A congestion control mechanism is said to be fair if the average transmission rate of each connection is approximately R/K, with R equal to the tranmission rate of a link and K equal to number of TCP connections on said link


Explicit Congestion Notification (ECN)
More recently, extensions to both IP and TCP have been proposed and implement to allow for network assisted congestion control. The network can now in these implementations explicitly signal network congestion to hosts. This form of network assisted congestion control is know as Explicit Congestion Notification
How it works is if a sender and reciever both have flagged their ECN bit showing they are ECN compatible, a router will send an ECN congestion notification along in its datagram that is going to the reciever. The reciever will include in its next ACK to the sender a flag on the ECE bit (Explicit Congestion Notification Echo) to say there is congestion. The sender then takes action by halfing its congestion window, then marking the CWR bit in its next packet to signal it has reduced its congestion window





Chapter 4 Network Layer Data Plane

The primary goal of the network plane is to move packets from a sending host to a receiving host
To acheieve this, two important network layer functions can be identified:
Forwarding (also refered to as switching) - when a packet arrives at the router's input link, the router's job is to forward that packet to the appropriate output link (still inside the same router) (data plane)
Routing - the network layer must determine the path taken by packets as they flow from two end systems. It uses routing algorithms to achieve this (control plane)
Forwarding is often implemented in hardware (a few nano seconds) and Routing in software (typically a few seconds)
A key element in every router is its forwarding table, which examines the value of a packet's header and uses them to index its forwarding table. That index directs the packet to the correct output link
SDN or software defined networking is a recent approach that takes the forwarding table calculations and removes them from the routers in a system, and instead implement their calculations in a remote controller that sends the tables to each connected router

The network service model defines the characteristics of end to end delivery that the network layer could provide. These services can include:
Guaranteed Delivery
Guaranteed Delivery with bounded delay
In order packet delivery
Guaranteed minimal bandwith
Security
And many more

The internet's network layer provides only a single service, called best effort delivery, which makes no guarantees about anything

A packet switch transfers a packet from input link interface to output link interface according to values in the packet's header fields
A subset of packet switches, called link layer switches, base their forwarding decision upon values in the fields of the link layer frame (making these switches link layer devices)
Another subset of packet switches is the router, which base their forwarding on decisions in the header field values in the network layer data gram (making these switches network layer devices)


Inside the router architecture there is:
Input ports perform a couple key functions. First, they terminate an incoming physical link to the router. Secondly, they perform link layer functions needed to interoperate with the link layer function on the other side of the connection. Third, they perform a lookup function, consulting the forwarding table to determine the output port to which its packet will be forwarded to via the switching packet. This part will also pass on control packets to the routing processor
The switching fabric connects the router's input and output ports (acts as their network)
Output ports stores packets recieved from the switching fabric and transmits these packets to the outgoing link by performing the neccesary link layer and physical layer functions. Often, these ports are bidirectional, meaning both ports will be performing each other's functions from time to time
The routing processor performs control plane functions, such as executing routing protocols and maintains the routing tables. In SDN routers, it is responsible for communicating with the remote controller

Destination based forwarding bases the output port on a packet's final destination
Generalized forwarding takes many more factors into consideration, such as the packets importance



Input Processing and Destination Based Forwarding
Input processing needs to be fast (less than a microsecond in many cases) to keep up with today's gigabit speeds. As such, input processing uses many different techniques to make that happen 

Forwarding table is copied from the processor to the line cards over a seperate bus. With these shadow copies at each input port, the port can make forwarding decisions locally without having to reference the processor on a packet by packet basis

Ternary Content Addressable Memories (TCAM) are often used for lookup tables. With TCAM, a 32 bit IP address is presented to memory, and returns the content of the forwarding table for that entry in almost constant time. THey can hold upwards of a million table entries

One example of a forward table with destination based forwarding is each output port is given a range of addresses. Then, you only need to match up the common prefix of the high and low range of the window to determine which port the packet goes too
The router will use the longest prefix matching rule if mutlipule ranges happen to have part of the same prefix

Although lookup is one of the more important steps that take place in input processing, other important functions reside here as well, such as physical and link layer processing, queuing of packets to be entered into the switching field, checking of a packet's version number, checksum, and time to live field, and counters used for network management

The match plus action behaviour of the input port is an abstraction that occurs in many other parts of networking, not just routers



Switching

Three main switching fabric types:
Switching via memory - input port signaled processor via interupt, packet was then copied into processor memory, where destination address was addressed and processed, followed by being forwarded to the appropriate output port. Each packet takes up the entire attention of the processor/memory, making it so only one packet from the collection of all ports can be forwarded at a time
Switching via a bus - port transfers packet directly to output port without intervention by the processor. All output ports recieve a packet, but only one accepts the packet based on header info, the others drop the packet. Similar to memory though where only one packet can travel the bus at a time
Switching via an interconnection network (crossbar) - fabric is set up like a bus except there are 2N buses for every N ports. This allows for, lets say, input port A to send packets to output port Z at the same time B is sending to Y. Vertical buses intercept horizontal buses, and these intersects can be closed/opened at any time


Output Port Processing

Packet loss occurs at a port's queueing (buffer management) stage when a new packet arrives at a moment when the buffer is already full. The port must discard either the incoming packet (drop tail) or another packet in buffer when this happens
This can occur at the input port for example in a crossbar switching router, when two input ports are trying to send information to the same output port. Since only one can send to an input port at a time, one must wait, all the while filling up their input buffer more from incoming packets. 
When a packet at the front of a buffer is preventing packets behind it from reaching their output port (say packet 1 is waiting for output port 1 to be ready, but packets 2-100 want to go to output port 3, which is open, but can't because of packet 1), it is callsed head of the line (HOL) blocking

AQM, or active queue management, are techniques that aim to proactively solve packet dropping One of the most widely used AQM's is Random Early Detection (RED)

When calculating the amount of buffer needed for these input and output ports, the old rule was usually B = RTT * C (link capacity, such as 10 Gbps)
The new accepted rule is B = RTI * C/srqt(N)


Packet Scheduling
Packet scheduling deals with the order of which queued packets are transmitted over the network
First in First Out (FIFO), also knows as First come First Served (FCFS) is pretty self explanatory
Priority queueing divides packets into subgroups, and gives those certain subgroups priority over other groups in being sent out first. Under non-preemptive priority queueing, the transmission of a "lesser" packet will not be stopped if, during its processing, a high priority packet arrives
Round Robin and Weighted Fair Queuing (WFQ) use concepts from priority queuing, but give certain subgroups different weights, guaranteeing them a bandwith that is a function of their weight and the link capacity
Work conserving queuing will never allow the link to remain idle when there are packets of any class queued for transmission





The Internet Protocl (IP)

The internet's network layer packet is refered to as a datagram
The datagram for IPv4 consists of the follwing header lines:
Version number (4 bits) - specifies the IP protocol version of the datagram (IPv4, IPv6, etc.)
Header length (4 bits) - needed because the options field, which is apart of the header, can contain a range of bits (often times 0). Determines at what point the payload (transport layer segment being encapsulated) exists
Type of service (8 bits) - allows different types of datagrams to be distinguished, and certain actions to be taken upon these different types (such as real time vs non real time datagrams)
Datagram length (16 bits) - total length of the header lines plus data in bytes is stored here. Since this line is 16 bits, the maximum size of an IPv4 datagram is 65,535 bytes. In practice, datagrams are rarely larger than 1,500 bytes
Identifier, flags, and offset (16 bits, 3 bits, 13 bits) - these deal with fragmentation (discussed later). IPv6 does not allow for fragmentation
Time to live (8 bits) - (TTL), ensure that datagrams do not circulate forever. Decremented each time is processed by a router. If it reaches 0, the datagram is dropped by the router
Upper layer protocol (8 bits) - indicates the specific transport layer protocl to which the data portion of the IP datagram is passed too (EX: 6 for TCP, 17 for UDP)
Header Checksum (16 bits) - aids routers in detecting bit errors in a recieved datagram. Works the same as transport checksums. Routers typically drop error ridden datagrams
Source and destination IP addresses (32 bits each) - self explanatory
Options - self explanaotory. Are not included in IPv6 due to extra information/processing time needed for them to exist
Data/payload - carries the transport layer segment 
A total of 20 bytes of header information not including if there are any options


IPv4 Datagram Fragmentation
The maximum amount of data a link layer frame can carry is called the maximum transmission unit (MTU). For ethernet, this is 1,500 bytes of data
The MTU places a hard limit on the size of a datagram packet, and the MTU can be different at each stage of the datagrams journey
Dividing up a datagram into smaller datagrams to meet these requirements is called fragmentation
Since the transport layer will not accept fragmented packets, they must be unfragmented at some point. This is done at the end system's network layer to keep the network core as simple as possible
Identifier tells you the number of the datagram in order of the fragmented packet. Flag bit set to 1 unless it is the last fragment, then its set to 0. Offset field specifies where each fragment fits within the original datagram


IPv4 Addressing
An IP address is techincally associated with an interface (the boundarty between the host and physical link) rather than the host or router containing that interface (this is kinda dumb but might be a test question)
These addresses are usually written in dotted decimal notation, with each byte value being seperated by a dot (EX: 192.168.0.1)
Each interface on every host and router in the global internet must have an IP address that is globally unique (except for interfaces behind NATs). A portion of an interface's IP address will be be determined by its subnet/IP network
IP addressing assigns address to a subnet such as giving it the subnet mask 223.1.1.0/24 . The slash 24 is the subnet mask that indicates the left most 24 bits of the 32 bit define the subnet address. Therefore, in a slash 24 subnet, there can be addresses between 223.1.1.0 and 223.1.1.255

The internet's assignemnt stratergy is know as Classless Interdomain Routing (CIDR). CIDR assigns an organization a block of contigious addresses; addresses where their prefix is identical (a.b.c.d/x for some value x). This helps router's forwarding tables outside an organizationn because often times they will only need to consider the prefix of a address (known as address aggregation/ route aggregation / route summarization)
The remaining 32-x bits distinguish the interfaces within the organization
Before CIDR was adopted, network portion (prefixes) of an IP address were constrained to be 8, 16, or 24 bits in length (this was called classful addressing)
This was a problem because IP networks could only support a set number of end 
systems (254 for /24, 65,634 for /16) (2 addresses are reserved for special use, otherwise itd be 256 and 65636). This led to a rapid deletion of address space
The IP brodcast address, 255.255.255.255, is reserved for a signal to send the message to all hosts on the same subnet

A unicast address is used to refer a single host
A multicast address is used to deliver a packet to a group of hosts

Obtaining a Block of Addresses
ISP's can designate subsets of their own subnet to an organization, but how does the ISP obtain its subnet?
The Internet Corporation for Assigned Names and Numbers (ICANN), is a nonprofit organization that allocates IP addresses and manages root DNS servers. It also assigns domain names and resolves domain name disputes
ICANN allocates addresses to regional internet regestries (ARIN, RIPE, APNIC, LACNIC), and those registries handle the allocation of addresses within their region

Dynamic Host Configuration Protocol (DHCP)
A system admin will usually manually configure IP address for router interfaces in its organization. However, for end system interfaces, addresses can be configured automatically with Dynamic Host Configuration Protocl (DHCP)
DHCP is refered to as a plug and play or zeroconf protocol

DHCP can be configured to give a host the same IP address every time it connects to the network, or it may be assigned a temporary IP address that will be different each time it connects to the network

DHCP is a client server protocol, where a client is a newly arriving host wanting to obtain network config information, and the server is a DHCP server used across the subnet to assign and configure this information for the client (can be inside the router as well)

For a newly arriving host, the DHCP protocol is a four step process:
1. DHCP server discovery - the host sends out a DHCP discover message with UDP to port 67. Since it has no information on the subnet's DHCP server, it sends this message to 255.255.255.255 (which sends its to all of the subnet), and sets its address as 0.0.0.0 . 	
2. DHCP server offer(s) - DHCP recieves a discover message and responds to the client with a DHCP offer message and an address lease time. This is also broadcast to 255.255.255.255 . There could be multipule DHCP servers, so the client might get multipule offers
3. DHCP request - client reads offer(s), chooses one, and responds to that client with a DHCP request message to recieve config parameters
4. DHCP ACK - server responds to request message with a DHCP ACK message including parameters



Network Address Translation (NAT)

The address spaces 10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16 are the three portions of the IP address space that is reserved for a priavte network or realm with prtivate addresses
Millions of devices have these IP addresses, therefore when a host references once of these, it is a given that they are referencing a part of their subnet
A NAT enabled router does not look like a router to the outside world; instead it looks as if it is a single device with a single IP address
NAT routers use a NAT translation table to direct traffic to the correct hosts in its subnet

A host on the subnet sends a datagram as normal to its router. The NAT router recieves the datagram, generates a new source port number, replaces the source IP with its own, and sends it out to the Internet. When a response comes back from the destination IP containing the NAT's IP and its randomly chosen port number as the destination port, the NAT router will be able to refer to its translation table to see which IP and port it assigned that random port too, and forward it acordingly. Because the port field is 16 bits, a NAT router can hold 60,000 simultaneious connections with a single IP address

NAT has some problems since it is using ports for a purpose they were not intended for (directing to host and processes rather than just directing to processes) 
Servers listening on certain ports and peers that also need to listen on certain ports are going to have problems
NAT traversal tools (RFC 5389) like Universal Plug and Play (UPnP) is a protocol that allows a host to discover and configure a nearby NAT

NATs and other middleboxes operate in a network layer but have quite different functions from routers, such as NAT, load balancing, traffic firewalling, and more



IPv6
Started being developed in the 1990's once it became clear unique 32 bit addressess were going to be exhausted soon
The IANA allocated its last remaining pool of unassigned IP addresses in 2011

Enhancements made to the IPv6 datagram:
Expanded addressing capabilities - increased the size of the IP address from 32 bits to 128 bits. Also introduced a new type of address, an anycast address, that allows a datagram to be delievered to any one of a group of hosts
Streamlined 40 byte header - by removing the options field, a IP datagram will now always be 40 bytes long, removing the need for some of the old header fields
Flow labeling (20 bits) - allows labeling of packets to paticular flows for which the sender requests special handling, such as a real time service. Basically a way to give packets better treatment
Payload length (16 bits) - gives the size of the datagram minus the fixed 40 byte header size
Next header (8 bits) - does what the protocol field did in IPv4
(Figure 4.26 on page 378)

Fields that IPv6 deleted from IPv4:
Fragmentation - fragmentation can not be done with IPv6. If an IPv6 datagram is too large for a router, it will simply send a Packet Too Big ICMP error message back to the sender. It then becomes the sender's job to resend the data using smaller datagrams
Header Checksum - developers thought this redundant since the link/transport layers perform the same operation, so it was removed
Options - explained earlier


Transitioning from IPv4 to IPv6
IPv6 systems are backwards compatible, but IPv4 machines are not compatible with IPv6
The approach to the IPv4 to IPv6 most widely adapted is tunneling
Say you have a set of IPv6 hosts connected by IPv4 routers. The IPv6 hosts can encapsulate their IPv6 datagram into the payload of an IPv4 datagram, continue as an IPv4 datagram, then decapsulate to IPv6 once it reaches the IPv6 compatible device. To signal a IPv4 datagram with an IPv6 datagram as the payload, a value of 41 is put in the IPv4 datagram protocl number field


Generalized forwarding and SDN
In generalized forwarding, a match plus action table matches values of multipule header fields to determine the action that is made with the datagram; this can include forwarding the packet to one or more outputs, load balancing packets across multipule interfaces, rewritting header files (NAT), dropping a packet, and sending a packet to a server for special processing (DPI)
These devices are refered to as packet switches, and can be made using network and/or link layer properties
The following information about generalized forwarding will be based on OpenFlow (ONF 2016)

Each enrty in a match plus action table, known as a flow table, includes:
A set of header field values to which an incoming packet will be matched. A packet that doesn't match any flow table entry can be dropped or sent to DPI
A set of counters that track the number of packets that have been matched to a table enrtry
A set of actions to be taken when a packet matches a flow table entry

The flow table is essentialy an API for controlling an individual packet switch's behavior


Match
OpenFlow can match a variety of different packet fields from many different layers
For example, OpenFlow 1.0 can address header information in a link layer packet, datagram, and segment (figure 4.29 page 385), three different layers
While some information about headers can be matched to a flow table. OpenFlow does not allow all of them too be because of worries about bloating the system


Action
If multipule actions match a flow table entry, they are taken in this order:
Fowarding - either to one, many, or multicast over a subset of ports. Also can be forwarded to the processor for adjustments
Dropping - A flow table with no action indicates that the packet should be dropped
Modify Field - In OpenFlow 1.0, ten different values in the header fields for link, network, and transport layer can be overwritten

Flow tables and packet switching are the keys t many functions such as load balancing, firewalls, and virtual networks
Examples of flow table actions are on page 388






Chapter 5 Network Layer Control Plane


Routing Algorithms

A centralized routing algorithm computes the least cost path between a source and destination using complete knowledge about the network. Often refered to as link state algorithms when they have global knowledge of the entire network
A decentralized routing algorithm has the calculation of the least cost path carried out in an iterative, distributed manner by each router. No node has complete information about the entire network. Instead, neighboring nodes communicate with each other to find the shortest path route

Static routing algorithms change very slowly over time (only due to manual intervention), while dynamic routing algorithms change routing paths based on network traffic loads/topology change

In a load sensitive algorithm, edge (link) costs vary dynamically to reflect network congestion. Load insensitive algorithms do not change link costs based on network congestion


The Link State Routing Algorithm

A link state broadcast algorithm involves all nodes in a network broadcasting link state packets to each other, with each link-state packet containing the idenities and costs of all its attached links. This gives each node in the network a global view of the entire network
An example of a link state routing algorithm is Dijkstra's Algorithm (pg 408 and paper notes)


The Distance Vector Routing Algorithm

This algorithm is iterative, asynchronous, and distributed. Each node recieves information from one of its neighbors, performs a calculation, and distributes the results of its calculations back to its neighbor
This way costs/distances are constantly updated and the forwarding table values slowly converge to the optimal paths (pg 413)

Distance vector differs from an LS algorithm in the fact that it is not centeralized. It does not need global information of the network to get started, only its costs to its neighbor nodes

A routing loop is where there is an error in cost evaluations and two nodes try to route to a 3rd node through each other, resulting in an endless loop
Count to infinity problem is where a routing loop takes a long time to resolve since it will naturally increment to the correct cost slowly

Poisoned reverse is a technique used to avoid routing loops. If a node x uses y to get to z in its shortest path algorithm, then it will advertise its cost to z as infinity to node y only. Since y believes x has no path to z, it will never attempt to route itself to z through x (and thus will not create a loop)

Comparissons between DV and LS on page 419


Intra-AS ROuting in the Internet: OSPF

To combat scaling and provide administrative autonomy, autonomous systems (ASs) are used by organizations. All routers in an AS run the same routing algorithm and are addressable by their collective AS address (also given out by the ICANN, just like ISPs)

Open Shortest Path First (OSPF) is an open source link state protocol that uses flooding of link-state information and a Dijkstra's least cost path algorithm. Each router under an AS that runs OSPF contstructs a topological map of the entore autonomous system. Each router then runs its own calculations to determine the shortest path to all subnets in the AS. 


Routing Among the ISPs: BGP

Inter-autonomous system routing protocols are what transfer data between two different autonomous systems
All ASs must run the same inter-system routing protocol for this to work. For the internet, this protocol is called Border Gateway Protocol (BGP)
BGP is up there in most important internet protocols right along with IP
It is a decentralized, asynchronous, distance vector protocol (like Bellman's algorithm discussed earlier)

BGP provides each router the means to obtain prefix reachability information from neighboring ASs and determine the best routes to those prefixes

In BGP, pairs of routers communicate over TCP connections via port 179

A gateway router is a router on the edge of an AS that connects to the "outside world", and an internal router is a router which connects directly to only hosts in its own AS

A BGP connection between gateway routers of different AS's is called an external BGP (eBGP) connection, whilst one encapsulated in an AS is called an internal BGP (iBGP) connection


Involved with a subnet prefix in a BGP advertisement are several BGP attributes. The attributes and and prefix together are called a route

The AS-PATH attribute contains the list of ASs through which the advertisement has passed, and the NEXT-HOP attribute is the value of the IP address that stands at the beggining of the AS-PATH



Hot Potato Routing

The goal of hot potato routing is to get packets out of one's own AS as quickly as possible, without worrying about the cost of the remaining links left to the packets destination


Route Selection Algorithm (BGP's)

For any given destination prefix, the input to BGP's route selection algorithm is the sret of all routes to that prefix that have been learned by the router

Routes with higher local preference values (configed by system admins) will be favored over others. From these high preference routes, the route with the shortest AS-PATH is selected (least amount of ASs traveled through, distance vector algorithm). If there is still ties, hot potato routing is then used




IP-Anycast

IP-Anycast takes advantage of the internet's BGP. It is the process of giving multipule different servers geographicly distributed the same IP address, then using BGP's routing protocl to have clients communicate with the server closest to them (fewset number of ASs away). Also used to help DNS server connections (there are actually only 13 root DNS IP addresses, but over thousnads of actual servers spread out across the world)



Routing Policy

A multi-homed access ISP is an ISP that is connected to a network by more than one provider

A rule of thumb for provider networks is that any traffic flowing across an ISPs backbone must have either a desintation or source within that ISP's network. If not, that means that some other ISP is passing off their duty to them to give the packet a "free ride"


Software Defined Networking in the Control Plane

Four key characterisitcs of a SDN architecture are:
1. Flow based forwarding - forwarding decisions based on packet information (header fields)
2. Seperation of data plane and control plane
3. Network control functions - monitor and control key network functions
4. A programmable network - APIs that calculate routing, packet decisions, etc


A SDN's controllers functionality can be divided into three layers:
1. Communicating layer - communicating between the SDN controller and network devices. Know as the controllers southbound interface, most controllers use OpenFlow for this communication
2. Network wide state-management layer - gathers and stores up to date information about network hosts, links, switches to determine flow tables  for the various controlled devices
3. Interface with the network control application layer - know as the controller's northbound interface, allows it to communicate with its network control applications, which read/write to network state and flow tables in the management layer


OpenFlow Protocol

OpenFlow involves communication with the SDN controller and a SDN controlled switch. It uses a TCP connection and defaults to port number 6653

The controller communicates to the switch:
- Configuration of a switch
- Modify State to add/delete enteries in a switch's flow table, and set port properties
- Read State to collect information from a switch's flow table and ports
- Send Packet to send a specific packet to the switch at a given port

The switch communicates to the controller:
- Flow Removed, that an entry from its flow table has been removed due to timeout/ modify state message, etc.
- Port Status which informs the controller of a status change in a port
- Packet In when a packet arrives but doesn't have a corresponding flow table entry, is sent to the controller for additional processing

OpenDayLight and ONOS controllers are some of the most popular open source SDN controllers today



The Internet Control Message Protocol (ICMP)

Used by hosts and routers to communicate network information between each other
ICMP messages are carried by datagram payload, just like TCP/UCP is (upper layer protocol number of 1 for header field)

ICMP messages have a type and a code field, and contain the first 8 bytes of the IP datagram that caused the error (for packet recginiton by the sender)
ICMP code table on pahe 448



Network Management and SNMP

The key components of the network management framework are
- The managing server - an application that controls the collection, processing, and analysis of network information. Located in the network operations center (NOC) and heavily interacted with humans
- Managed device - a piece of networl equpiment that is connected to the managed network. Includes hosts, routers, switches, middlebox, modems, IoT devices, and anything else that is network connected. Usually several managed objects are in each managed device, which are pieces of hardware and/or configuation parameters for hardware/software
- Management Information Base (MIB) - information from managed devices/objects, specified in a data description language called Structure of Management Information (SMI)
- Network Management Agent - located in each managed device, communicates with the managing server and takes local actions under the command of the server
- Network Management Protocl - protocol used between all these devices that provides the capabilities of the network management to monitor, test, poll, configure, analyze, evaluate, and control



Simple Network Management Protocol (SNMP)

Application layer protocl used to convey network management and control messages between a managing server and managed device

Most common use is a request response scenario, where the managing server sends a request to a SMNP agent (pertaining an MIB object(s)), and that agent takes some action then responds. The agent can also send the server an unsolicited trap message, which it performs whenever an exceptional situation occurs (EX a link interface going up/down)

Seven Type of SNMP messages,  or protocol data units (PDUs), table on pg 452
Get Request
Get Next Request
Get Bulk Request
Inform Request
Set Request
Response
SNMPv2-Trap

SNMP packet structure on pg 453

SNMP default transport layer protocol is UDP, therefore there is no guarantee a managing server's requests will reach their intended agents
There is a request ID however that is incremented each time a request is sent to a specific agent, which will help communicate whether packets are being lost
SNMP has evolved through three versions. The newest version, SNMPv3, has more of a focus on providing additional security and administration capabilities, since SNMP in the past has been insecure, therefore often times only being used for its monitoring capabilites and not its administrative ones




